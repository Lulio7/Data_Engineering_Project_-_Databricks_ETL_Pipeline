# ğŸ“Š Data Engineering Project â€“ Databricks ETL Pipeline

## ğŸ“Œ Project Overview
Este proyecto muestra el desarrollo de un **pipeline de datos end-to-end en Databricks**, utilizando una arquitectura **Medallion (Bronze â†’ Silver â†’ Gold)**.  
El objetivo es simular un flujo real de procesamiento de datos, desde la ingesta hasta la preparaciÃ³n de datos listos para anÃ¡lisis.

---

## ğŸ¯ Objetivo del proyecto
- Simular un **Data Lake** mediante volÃºmenes en Databricks  
- DiseÃ±ar un pipeline ETL escalable y ordenado  
- Aplicar validaciones y transformaciones sobre los datos  
- Preparar informaciÃ³n estructurada para consumo analÃ­tico  

---

## ğŸ§° TecnologÃ­as utilizadas
- **Databricks**
- **Apache Spark (PySpark)**
- **Delta Lake**
- **Unity Catalog**
- **SQL**
- **Git / GitHub**

---

## ğŸ§± Arquitectura del proyecto

### **Bronze Layer**
- CreaciÃ³n de volÃºmenes para simular un Data Lake  
- Carga de archivos crudos (raw data)  

### **Silver Layer**
- Ingesta de datos:
  - Ventas  
  - Productos  
  - Clientes  
  - Sucursales  
- Validaciones:
  - EliminaciÃ³n de duplicados  
  - EliminaciÃ³n de valores nulos  
- Casteo de tipos de datos  
- Filtrado de registros innecesarios  
- OptimizaciÃ³n de joins  
- CreaciÃ³n de tablas Silver  

### **Gold Layer**
- CreaciÃ³n de tablas finales optimizadas para anÃ¡lisis  

### **OrquestaciÃ³n**
- ConfiguraciÃ³n de un **Databricks Job** para automatizar el proceso ETL de ventas  

---

## ğŸ“‚ Estructura del repositorio

ğŸ“ project-root
â”‚
â”œâ”€â”€ ğŸ“ notebooks
â”‚ â”œâ”€â”€ bronze/
â”‚ â”œâ”€â”€ silver/
â”‚ â””â”€â”€ gold/
â”‚
â”œâ”€â”€ ğŸ“ data
â”‚ â””â”€â”€ (archivos utilizados en la ingesta)
â”‚
â”œâ”€â”€ ğŸ“ jobs
â”‚ â””â”€â”€ etl_ventas_job.json
â”‚
â””â”€â”€ README.md


ğŸ“Œ **Nota:**  
Los archivos utilizados para la ingesta de datos se encuentran dentro de la carpeta **`/data`** del repositorio.
